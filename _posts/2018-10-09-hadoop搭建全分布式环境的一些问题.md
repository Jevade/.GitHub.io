---
layout:     post
title:      hadoop
category: blog
description: hadoop 配置的问题
tag:hadoop ,linux
---
# 全分布式的hadoop环境的搭建
在搭建中设计到的几个文件如下
1. hadoop-env.sh
	   在脚本中配置JAVA_HOME的路径_
2. hdfs-site.xml
	   在文件中配置dfs的冗余度和权限设置，冗余度最多为3，一般和节点数一样
3. core-site.xml
	配饰fs.defaultFS ,一般为hdfs://\<hostname\>:9000
	配置hadoop.tmp.dir ,hadoop临时文件存储的地方，默认在/tmp目录下，需要更改到其他适合的目录。
4. maprep-site.xml
5. yarn-site.xml
	 配置yarn主节点的位置，以及nodemanage执行任务的方式
6. slaves
	 配置集群中的slave，只需要把slave节点的host写入就可以了
	 比如/etc/hosts 里面如下：
	1.1.1.0 master
	1.1.1.1 slave1
	1.1.1.2 slave2
则slave中配置为：
	slave1
	slave2

# 出现的问题
主节点启动成功，但是127.0.0.1:50777UI页面上查看不到datanode,在shell上使用jps查看，主节点上正常，salve节点上柱出现datanode，nodemanager时常不出现。
1. yarn-site.xml配置有问题
在主节点上使用start-all.sh启动集群的时候，能看到master上各个功能模块都启动了，但是salve上使用jps查看，nodemanager经常一闪而过，只有datanode服务，并且使用stop-all.sh关闭集群式，shell上提示no datanode to stop，通过查看日志，发现salve在于master的8031端口连接时，连接出错，链接的地址是0.0.0.0:8031。
	INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
日志中大量出先上面的记录，因此修改了配置文件，改为如下配置：
	<property>     
	 <name>yarn.resourcemanager.address</name>      
	<value>Master:8032</value>  
	</property> 
	<property>    <name>yarn.resourcemanager.scheduler.address</name>      <value> Master:8030</value>  
	</property>
	<property>    
	<name>yarn.resourcemanager.resource-tracker.address</name>      <value> Master:8031</value>  
	</property>

2. 防火墙没有关
同时发现修改之后，ip地址链接是对的，但是依然无法链接，尝试关闭了防火墙：
	systemctl status iptable `
	systemctl stop iptables`
链接成功，集群顺利起来。在管理UI上可以看到节点信息。

 



